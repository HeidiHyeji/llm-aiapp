# ✅ 8.3.3 추측 디코딩 (Speculative Decoding)
> LLM 추론 속도를 2배 이상 높일 수 있는 혁신적인 기술
---

## 🧠 기본 아이디어

> **작은 모델(Draft model)**이 먼저 여러 토큰을 "추측"해서 만들어주고,
> **큰 모델(Target model)**이 그 결과를 **검증하고 일부만 수정**하는 방식

📌 목표는?
✔ 정확도는 그대로 유지하면서
✔ 속도는 2배 이상 빠르게!

---

## 🔄 전체 구조 흐름

1. **Draft 모델이** 예측된 토큰을 먼저 생성 (예: 4개 토큰)
2. **Target 모델이** 이 토큰들을 다시 평가
3. 3가지 경우로 나뉨:

| 경우      | 설명                           |
| ------- | ---------------------------- |
| ✔ 전부 승인 | 모두 정확함 → 그대로 사용 (최고 속도)      |
| ✔ 일부 승인 | 앞부분만 맞고 뒤는 수정                |
| ❌ 전부 거부 | 다시 전부 생성 (속도 저하 있지만 정확도는 유지) |


---
## 🔎 예시
### 📘 프롬프트

```
Prompt: "인공지능은"
```

---

### 1️⃣ **Draft 모델의 예측**

작은 Draft 모델이 빠르게 아래와 같은 토큰을 생성합니다:

```
[인공지능은] → [사람처럼] → [사고하고] → [학습하며] → [문제를]
```

📌 즉, Draft는 최대 4개까지 "추측"해서 생성함

---

### 2️⃣ **Target 모델의 검증**

큰 Target 모델이 이 추측 결과를 검토합니다:

| 위치 | Draft 토큰 | Target 결과 | 승인 여부   |
| -- | -------- | --------- | ------- |
| 1  | 사람처럼     | 사람처럼      | ✔ 승인    |
| 2  | 사고하고     | 사고하고      | ✔ 승인    |
| 3  | 학습하며     | **생각하고**  | ❌ 거절    |
| 4  | 문제를      | (검증 불가)   | (새로 생성) |

---

### 3️⃣ **최종 결과 구성**

* **앞의 두 토큰은 승인** → 그대로 사용
* **3번째 이후는 재생성**

📌 Target 모델은 "생각하고", "이해한다", ... 등의 새 토큰을 이어서 생성함

---

#### ✅ 최종 출력

```
[인공지능은 사람처럼 사고하고] → [생각하고 이해한다...]
```

* 앞 2개: Draft 승인
* 뒤: Target 새로 생성

---

### 📊 속도 절약 포인트

| 단계          | 설명                       |
| ----------- | ------------------------ |
| Draft 모델 생성 | 매우 빠르게 4개 토큰 생성          |
| Target 검증   | 첫 2개 승인 → 검증만으로 해결       |
| 재생성 범위      | 3번째 이후만 새로 생성 → 전체 속도 향상 |

---

### 🔁 그림 8.28과 비교해 보면:
<img width="290" alt="image" src="https://github.com/user-attachments/assets/fa36223d-1831-4af6-8ba6-74958995cd7c" />

* Draft 모델이 먼저 "샘플링"
* Target 모델이 "검증" (accept/reject)
* 일부 승인, 일부 거절 → **최종 결과는 섞어서 사용**

---

## ⚡ 왜 빠른가?

* 큰 모델은 느리지만 정확함
* 작은 모델은 빠르지만 부정확함
  → 작은 모델로 **미리 예측해두고**, 큰 모델이 **검증만 하게 하면** 전체 속도가 급상승!

---

## 📈 성능 효과

| 항목     | 기존 디코딩 | 추측 디코딩              |
| ------ | ------ | ------------------- |
| 속도(ms) | 14.1   | **7.5 이하**          |
| 품질     | 동일     | 동일                  |
| 모델 활용  | 1개     | 2개 (Draft + Target) |
| 비용     | 약간 증가  | **성능 대비 매우 효율적**    |

---

좋은 질문이에요!
책의 마지막 부분에서 언급된 \*\*“메두사(Medusa)”\*\*는 추측 디코딩(speculative decoding) 기법을 **한 단계 더 발전시킨 새로운 디코딩 방식**입니다.

---

## 🐍 Medusa란?

> **"추측 디코딩을 더 정교하게 확장한 구조"**
> → Draft 모델이 아니라 **Target 모델의 내부 구조를 확장해서**
> → **여러 후보 토큰(branch)**을 **동시에 예측**하고 평가하는 방식입니다.

---

## 📌 이름의 유래

* **Medusa**는 그리스 신화 속 괴물로 **머리가 여러 개의 뱀**으로 되어 있죠 🐍
* 여기서 착안해, **토큰 생성 시 여러 방향으로 “뻗어 나가는” 구조**를 만들었기 때문에 Medusa라고 부릅니다.

---

## 🧠 핵심 아이디어

기존의 추측 디코딩은:

* Draft 모델 → 여러 토큰 → Target 모델이 **검증**

**Medusa는:**

* Target 모델 안에 있는 **"Medusa Head" 구조**가
* 한 번의 forward pass에서 **여러 후보 토큰(branch)**를 **동시에 생성 & 평가**

---

## 🧩 어떻게 작동하나?

1. Target 모델은 매 토큰마다 **Top-K 후보 토큰**을 예측함
   (예: "사람처럼", "생각하고", "판단하며")

2. 각 후보에 대해 **다음 토큰을 병렬로 예측**
   → 마치 "여러 개의 Draft 경로"를 한 번에 만든 것처럼 동작

3. 최종적으로 가장 유망한 경로를 **선택 또는 재검토**
   (잘못된 경우 fallback 가능)

---

## 📈 장점

| 항목  | 내용                                 |
| --- | ---------------------------------- |
| 속도  | Draft 모델 없이도 **추측 병렬화** 가능 → 속도 향상 |
| 품질  | Target 모델 기반이므로 정확도 손실 없음          |
| 리소스 | 모델 1개로 해결 가능 (Draft 필요 없음)         |
| 구현  | 복잡하지만 **vLLM 등에서 실험 중**            |

---

## 📊 Medusa vs 기존 추측 디코딩

| 항목          | 추측 디코딩              | Medusa              |
| ----------- | ------------------- | ------------------- |
| Draft 모델 필요 | ✅ 있음                | ❌ 없음                |
| 검증 방식       | Target이 Draft 결과 확인 | Target이 직접 여러 후보 평가 |
| 속도          | 빠름                  | **더 빠름**            |
| 정확도         | 좋음                  | 좋음                  |
| 구현 난이도      | 보통                  | 조금 더 복잡             |

---

## ✅ 요약 문장

> **Medusa는 Draft 모델 없이, Target 모델 안에서 여러 추측 경로를 동시에 생성하고 평가하는 추측 디코딩 확장 기술이다.**
> 이름처럼 **여러 방향으로 뻗는 생성 경로를 한꺼번에 처리**해서 속도를 더 끌어올릴 수 있다.

---
