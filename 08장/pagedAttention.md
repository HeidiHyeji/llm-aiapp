# ✅ 8.3.2 페이저 어텐션 (PagedAttention)
> LLM 서빙에서 대규모 배치를 처리할 때 효율성을 극적으로 높이는 방법

## 🧠 문제 상황: KV 캐시 메모리 낭비

Transformer 디코더(예: GPT)에서 **토큰을 생성할 때**는 다음을 저장합니다:

> 각 토큰마다 생성된 **Key (K)**와 **Value (V)** → 이것들을 **KV 캐시**에 저장해 다음 토큰 생성 시 재사용

### 문제는?

* 입력 길이가 제각각인 여러 요청(batch)을 처리할 때,
* 미리 **최대 길이만큼 연속된 GPU 메모리를 잡아야** 함

📉 결과:

* 배치 중에 **짧은 요청**이 많으면 메모리 **낭비 심함**
* → 처리 가능한 **동시 사용자 수(배치 크기)가 제한됨**

---

## 💡 해결책: 페이저 어텐션 (PagedAttention)

> **KV 캐시를 연속 메모리가 아닌, "페이지(Page)" 단위로 블록처럼 쪼개서 관리**

### 핵심 아이디어:

* GPU 메모리를 가상 메모리처럼 사용
* **토큰 단위가 아닌 블록 단위(예: 128 토큰)**로 메모리 할당
* 블록은 페이지 테이블처럼 관리됨

➡️ 마치 OS가 RAM을 페이지로 관리하듯
➡️ **필요한 만큼만 동적으로 할당** & **낭비 없음**

<img width="450" alt="image" src="https://github.com/user-attachments/assets/63f60f9f-5b1e-47f5-977d-bfd7032ad79c" />

---

## 📦 구조 비유

| 기존 방식          | 페이저 어텐션            |
| -------------- | ------------------ |
| 노트 1권 통째로 미리 줌 | 낱장 노트 필요할 때마다 찢어 줌 |
| 토큰마다 다 메모리 잡음  | 여러 토큰 묶어서 "블록"으로 줌 |
| 남는 공간 많음       | **효율 최고**          |

---

## 📈 성능 효과 (책 기준)

| 항목          | 기존 KV 캐시  | 페이저 어텐션      |
| ----------- | --------- | ------------ |
| 지원 가능한 배치 수 | 8개 정도     | 32개 이상       |
| 메모리 활용률     | 낮음        | **높음**       |
| 처리 속도       | 느림 (낭비 큼) | **빠름 & 안정적** |

📌 GPT 류 모델처럼 길이 가변형 시퀀스를 생성할 때 매우 효과적입니다.

---

## ✅ 요약 정리

| 항목    | 설명                                                      |
| ----- | ------------------------------------------------------- |
| 목적    | 다양한 길이의 요청을 동시에 처리하면서도 메모리 낭비 최소화                       |
| 핵심 전략 | KV 캐시를 "페이지" 단위로 나눠서 관리                                 |
| 장점    | 메모리 절약, 배치 크기 증가, 서빙 효율성 극대화                            |
| 적용 예  | vLLM, Hugging Face Text Generation Inference 등 최신 프레임워크 |

