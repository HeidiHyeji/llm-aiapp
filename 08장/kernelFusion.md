# ✅ 8.3.1 커널 퓨전 (Kernel Fusion)

## 🧠 커널(Kernel)이 뭐야?
> Transformer 연산 속도를 올리는 아주 실용적인 최적화 기법
> GPU에서 어떤 연산을 수행할 때 사용하는 **작은 프로그램**이라고 보면 돼요.
> 예: `matmul`, `softmax`, `dropout` 같은 각각의 연산 = 각각의 GPU 커널

---

## ❌ 문제점: 연산마다 커널을 따로 실행하면?

Transformer 한 층에서 보통 아래 연산들을 차례로 수행합니다:

```
QKᵀ → 마스킹 → softmax → dropout → V 곱셈
```

### 이걸 따로따로 실행하면?

* GPU가 연산할 때마다:

  * **커널 로딩 → 실행 → 메모리 읽기 → 쓰기** 과정을 반복
* 즉, **연산은 빠른데**,
  \*\*매번 불필요한 메모리 입출력(IO)\*\*과 \*\*커널 전환 비용(Overhead)\*\*이 발생

➡️ 이게 누적되면 **전체 속도를 크게 떨어뜨림**

---

## ⚡ 해결책: 커널 퓨전 (Kernel Fusion)

> 여러 개의 GPU 연산(커널)을 **하나로 묶어서 한 번에 실행**하는 기술!

### 예시: 아래 연산을 모두 한 커널로 묶음

```
QKᵀ → 마스킹 → softmax → dropout
```

➡️ 메모리 접근을 최소화 + 커널 호출도 줄임

---

## 📈 효과 (책 그림 기준)

| 항목        | 기존 방식 | 커널 퓨전 적용 후   |
| --------- | ----- | ------------ |
| 커널 수      | 4개    | 1개           |
| 메모리 읽기/쓰기 | 4번    | 1번           |
| 속도        | 느림    | 최대 2배 이상 빨라짐 |

---

## 🎯 왜 효과적인가?

| 이점              | 설명                                 |
| --------------- | ---------------------------------- |
| **메모리 접근 감소**   | 중간 결과를 GPU에 저장하지 않고 바로 다음 연산에 전달   |
| **커널 호출 비용 감소** | GPU 연산 준비 시간이 줄어듦                  |
| **캐시 효율↑**      | 같은 block/thread 안에서 연산 계속하므로 성능 향상 |

---

## 🧠 비유로 쉽게 설명하면

| 비유               | 설명                                         |
| ---------------- | ------------------------------------------ |
| **음식 조리**        | 재료 다듬기 → 볶기 → 간하기 → 담기 → 각각 따로 조리사 바꾸는 것 ❌ |
| **퓨전**           | 한 명이 한 번에 다 처리! ✔️                         |
| → 빠르고, 도구도 적게 씀! |                                            |

---

## ✅ 요약 정리

| 항목    | 설명                                   |
| ----- | ------------------------------------ |
| 목적    | Transformer 연산에서 중간 연산을 하나로 묶어 속도 향상 |
| 효과    | 커널 수 줄고, 메모리 IO 줄어들어 최대 2배 이상 빨라짐    |
| 적용 대상 | softmax, dropout, QKᵀ 등 연속적 연산들      |
| 핵심    | 연산을 "묶어서 한 번에 실행"하는 GPU 최적화 기술       |

---
