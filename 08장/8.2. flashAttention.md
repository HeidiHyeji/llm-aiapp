**8.2.1 플래시어텐션** 부분은 핵심적으로 GPU구조를 이용해서 “기존 어텐션보다 빠르고 메모리를 덜 쓰는 어텐션”을 구현한 방법

## 🔍 배경: 기존 Softmax 계산의 병목

### 🔹 기존 Softmax 계산

$$
\text{softmax}(x) = \frac{e^{x_i}}{\sum_j e^{x_j}}
$$

* 이걸 모든 토큰에 대해 한 번에 계산하려면 →
  **QKᵀ (N×N)** 전부 계산 → 이걸 \*\*HBM (느린 GPU 메모리)\*\*에 다 올려야 함
* 긴 시퀀스 + 큰 배치라면? → 메모리 오버플로우 위험 or 느림
  
---

## 💡 FlashAttention의 핵심 아이디어

> **전체 행렬을 만들지 않고, GPU의 고속 메모리(SRAM)에 올릴 수 있는 작은 블록 단위로 계산한다.**

이때 중요한 것이 GPU의 **메모리 계층 구조**입니다:

---

## 📦 GPU 메모리 계층

| 계층                              | 속도           | 용도         |
| ------------------------------- | ------------ | ---------- |
| **SRAM (레지스터, 공유 메모리)**         | 매우 빠름 (몇 ns) | 계산 중간 값 저장 |
| **HBM (High Bandwidth Memory)** | 느림 (수백 ns)   | 큰 데이터 저장   |
| **DRAM / CPU 메모리**              | 매우 느림        | GPU 외부     |

![image](https://github.com/user-attachments/assets/d7f1d7a5-b3db-47c7-8220-e67aaa22fb54)

➡️ FlashAttention은 계산 도중에 필요한 데이터를 **SRAM 안에서 처리 가능한 블록 크기로 나눠서**
→ **HBM 접근을 줄이고**, 속도를 크게 올릴 수 있었던 겁니다.

---
![image](https://github.com/user-attachments/assets/315f96a2-87d1-4070-9646-d8a34443e313)

> ✅ **“SRAM 안에서 처리 가능한 블록 단위로 쪼개서 연산한다”**는 건
> ✅ **Softmax를 쪼개서 연산한다**는 것과 **같은 개념의 다른 표현**입니다.

---

### 🔸 FlashAttention 방식

#### 핵심 전략:

1. **Softmax를 작은 블록 단위로 나눠서 부분 합을 먼저 계산**
   → 이 단계는 **SRAM 안에서 수행** 가능
2. 각 블록에서 나눈 계산 결과를 **나중에 합쳐서 전체 softmax 값 계산**
   → 이때 softmax의 기본 성질(덧셈, 지수 연산의 결합성)을 활용

---

## 📌 예시: 수식으로 보면

![image](https://github.com/user-attachments/assets/3a4fcfd1-7518-4a72-b2cd-dae05f8c020a)

책의 그림 8.13처럼 softmax 분할 수식은 이렇게 표현되죠:

$$
\text{softmax}(X) = \frac{f(X)}{I(X)} = \frac{e^{x_i - m(X)}}{\sum_j e^{x_j - m(X)}}
$$

* 여기서 $m(X)$는 전체에서 가장 큰 값 (overflow 방지용/지수 계산(exponentiation)을 안정화하기 위해 **최댓값 보정 (`max`)**)
* FlashAttention은 이걸 여러 블록 $X_1, X_2$로 쪼개서 처리:(softmax를 **두 조각으로 나눠서 합치는 방식**을 사용)


$$
I(X) = I(X_1) + I(X_2), \quad \text{같은 방식으로 } f(X) = f(X_1) + f(X_2)
$$


➡️ 즉, Softmax 연산을 쪼개서 **부분 계산하고 나중에 합치는 게 가능하다**는 걸 수학적으로 보여주는 거예요.

---

## ✅ 요약 정리

| 관점    | 의미                                      |
| ----- | --------------------------------------- |
| 수학적   | Softmax의 분할 가능성 (부분 지수합 계산)             |
| 하드웨어적 | GPU의 빠른 SRAM 안에서 블록 단위 처리               |
| 둘의 관계 | **“Softmax를 쪼갠다” = “SRAM에 맞춰 블록 계산한다”** |

---

## ⚡️ 즉, 플래시어텐션이 해결한 것

플래시어텐션은 이 문제를 아래 방식으로 해결했어요:

### ① **Softmax를 쪼개서 계산**

기존에는 N×N 행렬을 **한꺼번에 softmax** 했지만,
→ 플래시어텐션은 **부분 단위로 softmax를 나눠서 계산**합니다.


> 이 방식 덕분에:

* 전체 softmax를 계산하지 않고도 **정확한 결과를 근사**할 수 있고,
* **한 번에 GPU 메모리에 올리지 않아도 됨** → 메모리 사용량 대폭 감소!
  
---

### ② **기록만 하고 역전파 때 계산**

* 기존에는 순전파(forward)와 역전파(backward) 모두에서 `QKᵀ`을 계산해야 했지만,
* 플래시어텐션은 순전파에서는 **중간값만 저장**하고,
* 역전파에서는 그걸 가지고 다시 `QKᵀ` 계산을 재현함.

→ 계산량은 약간 늘어났지만, 메모리 IO가 줄어서 **전체 속도는 오히려 빨라짐**

---

## 📊 실질적인 효과 (표 8.1 기준)

| 항목            | 기존 어텐션 | 플래시어텐션 |
| ------------- | ------ | ------ |
| 연산량(GFLOPS)   | 66.6   | 75.2   |
| 메모리 읽기/쓰기(GB) | 40.3   | 4.4    |
| 실행 시간(ms)     | 41.7   | 7.3    |

* 연산량은 오히려 증가했지만,
* 메모리 접근량이 **10분의 1 이하로 줄어들어서 실행 시간은 6배 이상 단축**됨.

---

## 🧩 요약: 플래시어텐션은 왜 빠른가?

| 전략         | 설명                        |
| ---------- | ------------------------- |
| Softmax 분할 | 전체가 아닌 **일부씩 나눠 계산**      |
| Max 보정     | **지수값 안정화**를 위한 보정        |
| 역전파 최적화    | **중간 결과만 저장하고 역전파에서 재계산** |
| IO 최적화     | **GPU 메모리 입출력 대폭 감소**     |

---
